# SmallFormer

A Reduced Transformer Architecture with Parameter Free Multi-Head Attention and PReLU Token Level Mapping

Paper Coming Soon
